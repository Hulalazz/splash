{"name":"SPLASH!","tagline":"Parallel Stochastic Learning on Spark","body":"# Introduction\r\n\r\nSPLASH is a general-purpose tool for immigrating stochastic algorithms to multi-node distributed systems. It provides a user-friendly interface for stochastic algorithm development. The main features of SPLASH are:\r\n\r\n* **Ease of Use**: To parallelize a stochastic learning algorithm, there is no need to devise a problem-specific distributed implementation. Most stochastic algorithms can be immigrated to SPLASH for parallel execution without modification. The user doesn't need to change the algorithm's tuning parameters.\r\n\r\n* **Fast Performance**: SPLASH workers won't synchronize until processing a large bulk of data. Thus, communication cost won't be a bottleneck on the algorithm's efficiency. It makes SPLASH substantially faster than other distributed schemes which require frequent synchronization.\r\n\r\n* **Integration with Apache Spark**: SPLASH is built on [Apache Spark](https://spark.apache.org/streaming/). it takes the resilient distributed dataset (RDD) of Spark as input and generates RDD as output. In addition, it is seamlessly integrated with other data processing tools in the Spark environment, including the [MLlib machine learning library](https://spark.apache.org/mllib/). \r\n\r\n# Install SPLASH\r\n\r\nTo install SPLASH, you need to:\r\n\r\n1. Download and install [Apache Spark](https://spark.apache.org/streaming/).\r\n2. Download the [SPLASH jar file](https://github.com/zhangyuc/splash/blob/gh-pages/target/scala-2.10/splash_2.10-0.0.1.jar?raw=true) and put it in your project classpath.\r\n3. Make SPLASH as a dependency when [submitting Spark jobs](http://spark.apache.org/docs/1.2.1/submitting-applications.html).\r\n\r\n# Quick Start\r\n\r\nWhen SPLASH is in your project classpath, you can write a self-contained application using the SPLASH API. Besides importing Spark packages, you also need to import the SPLASH package in scala, by typing:\r\n\r\n```scala\r\nimport splash.ParametrizedRDD\r\nimport splash.StreamProcessContext\r\nimport splash.ParameterSet\r\n```\r\n\r\nJust as every Spark applications, the first step is to create a dataset for the learning algorithm. SPLASH provides an abstraction called **parametrized RDD**. The parametrized RDD is very similar to the resilient distributed dataset (RDD) of Spark, but it has particular data structure for maintaining the parameters to be learnt. A parametrized RDD can be created by a standard RDD:\r\n\r\n```scala\r\nval paramRdd = new ParametrizedRDD(data)\r\n```\r\n\r\nwhere `data` is a standard RDD object containing your dataset. To execute the learning algorithm on the dataset, set a stream processing function `update` to the parametrized RDD by\r\n\r\n```scala\r\nparamRdd.setProcessFunction(update)\r\n```\r\n\r\nThe `update` function is implemented by the user. It takes four objects as input: a random seed, a weighted data entry in the dataset, the set of shared variables used by the stochastic algorithm and the set of local variables associated with the data entry. An exemplary implementation of the logistic regression update is:\r\n\r\n```scala\r\nval update = ( seed: Random, entry: (Int, Array[String], Array[Double]), weight: Double, sharedVar : ParameterSet,  localVar: ParameterSet ) => {\r\n  val y = entry._1\r\n  val x_key = entry._2\r\n  val x_value = entry._3\r\n  val learningRate = 20.0\r\n  \r\n  // get the shared variable value by key \"t\"\r\n  val t = sharedVar.get(\"t\")\r\n  var sum = 0.0\r\n  for(i <- 0 until x_key.length){\r\n    // get the shared variable value by key \"w:\"+x_key(i)\r\n    sum += sharedVar.get(\"w:\"+x_key(i)) * x_value(i)\r\n  }\r\n  \r\n  for(i <- 0 until x_key.length)\r\n  {\r\n    val delta = weight * learningRate / math.sqrt( t + 1 ) * y  / (1.0 + math.exp(y*sum)) * x_value(i)\r\n                 \r\n    // increase the shared variable by delta\r\n    sharedVar.update(\"w:\" + x_key(i), delta)\r\n  }\r\n  // increase the shared variable by weight\r\n  sharedVar.update(\"t\", weight)\r\n}\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}